import numpy as np
from typing import Tuple
from PyFHD.pyfhd_tools.pyfhd_utils import extract_subarray


def vis_extract_autocorr(obs: dict, vis_arr: np.array, time_average = True, auto_tile_i = None) -> Tuple[np.array, np.array]:
    """
    TODO: Docstring

    Parameters
    ----------
    obs : dict
        The dictionary generated by PyFHD containing metadata and data from the observation
    vis_arr : np.array
        The visibility array
    time_average : bool, optional
        _description_, by default True
    auto_tile_i : _type_, optional
        _description_, by default None

    Returns
    -------
    Tuple[autocorr: np.array, auto_tile_i: np.array]
        The first array is the auto-correlation visibilities and the second array is the unique tile values
    """
    # TODO: check if tile_A and tile_B 2D
    autocorr_i = np.where(obs["baseline_info"]["tile_A"] == obs["baseline_info"]["tile_B"])[0]
    if (autocorr_i[0].size > 0):
        auto_tile_i = obs["baseline_info"]["tile_A"][autocorr_i] - 1
        auto_tile_i_single = np.unique(auto_tile_i)
        auto_corr = np.zeros(obs["n_pol"])
        for pol_i in range(obs["n_pol"]):
            # TODO: check vis_arr shape again, compare against IDL
            auto_vals = vis_arr[pol_i, :, autocorr_i]
            if (time_average):
                auto_single = np.zeros([auto_tile_i_single.size, obs["n_freq"]])
                time_inds = np.where(obs["baseline_info"]["time_use"])
                for tile_i in range(time_inds[0].size):
                    baseline_i = np.where(auto_tile_i == auto_tile_i_single[tile_i])
                    baseline_i = baseline_i[time_inds]
                    if (time_inds[0].size > 1): 
                        auto_single[tile_i, :] = np.sum(extract_subarray(auto_vals, np.arange(obs["n_freq"]), baseline_i)) / time_inds[0].size
                    else:
                        auto_single[tile_i, :] = extract_subarray(auto_vals, np.arange(obs["n_freq"]), baseline_i)
                auto_vals = auto_single
            # TODO: Get size of auto_vals or do a python list of numpy arrays
            auto_corr[pol_i] = auto_vals
        if (time_average):
            auto_tile_i = auto_tile_i_single
        return auto_corr, auto_tile_i
    else:
        # Return auto_corr as 0 and auto_tile_i as an empty array
        return np.zeros(1), np.zeros(0)

def vis_cal_auto_init():
    pass

def vis_calibration_flag():
    pass

def vis_cal_bandpass():
    pass

def vis_cal_polyfit():
    pass

def vis_cal_combine(): 
    pass

def vis_cal_auto_fit():
    pass

def vis_cal_subtract():
    pass

def vis_calibration_apply():
    pass

def calculate_adaptive_gain(gain_list, convergence_list, iter, base_gain, final_convergence_estimate = None):
    """
    TODO: Docstring
    [summary]

    Parameters
    ----------
    gain_list : [type]
        [description]
    convergence_list : [type]
        [description]
    iter : [type]
        [description]
    base_gain : [type]
        [description]
    final_convergence_estimate : [type], optional
        [description], by default None
    """
    if iter > 2:
        # To calculate the best gain to use, compare the past gains that have been used
        # with the resulting convergences to estimate the best gain to use.
        # Algorithmically, this is a Kalman filter.
        # If forward modeling proceeds perfectly, the convergence metric should
        # asymptotically approach a final value.
        # We can estimate that value from the measured changes in convergence
        # weighted by the gains used in each previous iteration.
        # For some applications such as calibration this may be known in advance.
        # In calibration, it is expressed as the change in a
        # value, in which case the final value should be zero.
        if final_convergence_estimate is None:
            est_final_conv = np.zeros(iter - 1)
            for i in range(iter - 1):
                final_convergence_test = ((1 + gain_list[i]) * convergence_list[i + 1] - convergence_list[i]) / gain_list[i]
                # The convergence metric is strictly positive, so if the estimated final convergence is
                # less than zero, force it to zero.
                est_final_conv[i] = np.max((0, final_convergence_test))
            # Because the estimate may slowly change over time, only use the most recent measurements.
            final_convergence_estimate = np.median(est_final_conv[max(iter - 5, 0):])
        last_gain = gain_list[iter - 1]
        last_conv = convergence_list[iter - 2]
        new_conv = convergence_list[iter - 1]
        # The predicted convergence is the value we would get if the new model calculated
        # in the previous iteration was perfect. Recall that the updated model that is
        # actually used is the gain-weighted average of the new and old model,
        # so the convergence would be similarly weighted.
        predicted_conv = (final_convergence_estimate * last_gain + last_conv) / (base_gain + last_gain)
        # If the measured and predicted convergence are very close, that indicates
        # that our forward model is accurate and we can use a more aggressive gain
        # If the measured convergence is significantly worse (or better!) than predicted,
        # that indicates that the model is not converging as expected and
        # we should use a more conservative gain.
        delta = (predicted_conv - new_conv) / ((last_conv - final_convergence_estimate) / (base_gain + last_gain))
        new_gain = 1 - abs(delta)
        # Average the gains to prevent oscillating solutions.
        new_gain = (new_gain + last_gain) / 2
        # For some reason base_gain can be a numpy float in testing so putting in a tuple solves this.
        gain = np.max((base_gain / 2, new_gain))

    else:
        gain = base_gain
    # Is the plan to use gain_list?
    # vis_calibrate_subroutine doesn't use it so no point in this statement currently
    # gain_list[iter] = gain

    return gain